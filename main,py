import requests
from bs4 import BeautifulSoup
from colorama import Fore, init


class PageVideo:
    def __init__(self, anime_name, episode_number, page_link):
        self.anime_name     = anime_name
        self.episode_number = episode_number
        self.page_link      = page_link

    def __str__(self) -> str:
        return f'Anime: {self.anime_name} | Number: {self.episode_number} | PageLink: {self.page_link}'

class Video:
    def __init__(self, anime_name, episode_number, page_link):
        self.anime_name     = anime_name
        self.episode_number = episode_number
        self.page_link      = page_link

    def __str__(self) -> str:
        return f'Anime: {self.anime_name} | Number: {self.episode_number} | PageLink: {self.page_link}'

def GetUrl(htmltext:str):
    videos = []
    urls = []
    url = None
    strings = htmltext.splitlines()
    for string in strings: # Get strings with '<video' string
        if string.find('<video') != -1:
            videos.append(string)
    for video in videos: # For str with video tag, search 'src' argument
        args = video.split();
        for arg in args:
            if arg.find('src=') != -1:
                urls.append(arg)
    if len(urls) > 1:
        f = open(r'D:\\_Projects\\DLAnime\\UrlsToCheck.txt', 'w')
        for url in urls:
            try:
                f.write(f'{url}\n')
            except:
                print(Fore.RED + url)
    else:
        for a in urls:
            starturl = a.find('http')
            endurl = a.find('.mp4') + 4
            url = a[starturl:endurl]
    return url

def DownloadFile(name, url):
    name = name + ".mp4"
    r = requests.get(url)
    f = open( name, 'wb');
    
    for chunk in r.iter_content(chunk_size=255): 
        if chunk: # filter out keep-alive new chunks
            f.write(chunk)
    print ("Done")
    f.close()



def GetList(obj):
    if type(obj) is not list:
        return [obj]
    else:
        return obj

def GetSoup(url):
    html = requests.get(url).text
    return BeautifulSoup(html, 'lxml')

def GetTitle(string : str):
    string = string.removeprefix('Assistir')
    string = string[0:string.find('-')]
    string = string[0:string.find('Legendado')]
    return string

def GetEpisodeNumber(string : str):
    string = string[string.find('Episódio') + len('Episódio'):].split()[0]
    return string

# Step 1 - Get all pages with episode selections
def GetPagesUrl(url):
    counter = 0
    pages_urls = GetList(url)
    while (True):
        soup = GetSoup(pages_urls[counter])
        nextPage = soup.find_all('a', 'next')
        if ( len(nextPage) > 0 ):
            pages_urls.append(nextPage[0].get('href'))
        else:
            break
        counter += 1
    return list(dict.fromkeys(pages_urls)) # Return a list without duplicate urls

# Step 2 - Get all link from episodes page selections (return page video link)
def GetPageVideosUrl(url):
    pages_url = GetList(url)
    videos_url = []
    for page in pages_url:
        soup = GetSoup(page)
        videos = soup.find_all('a', 'col')
        for video in videos:
            videos_url.append(PageVideo(GetTitle(soup.title.text), GetEpisodeNumber(video.get('title')), video.get('href')))
    return videos_url

# Step 3 - Get url of the mp4 video
def GetVideoUrl(url):
    pages_video = GetList(url)
    
    for page in pages_video:
        soup = GetSoup(page)



init(autoreset=True)
urls = ['https://goyabu.com/assistir/spy-x-family-parte-2/',
        'https://goyabu.com/assistir/death-note-legendado/',
        'https://goyabu.com/assistir/black-clover/']

# Step 1
pages_urls = GetPagesUrl(urls)
for page in pages_urls:
    print(Fore.MAGENTA + page)

# Step 2
pages_video = GetPageVideosUrl(pages_urls)
for page in pages_video:
    print(page)



# html = requests.get('https://goyabu.com/videos/8166/').text
# url = GetUrl(html)
# print(url)
# if url != None:
#     DownloadFile('abc', url)

# html = requests.get('https://goyabu.com/assistir/death-note-legendado/').text
# html = requests.get('https://goyabu.com/assistir/spy-x-family-parte-2/').text
# soup = BeautifulSoup(html, 'lxml')
# print(soup.title)
# print('\n\n')
# print(soup.p)
# print('\n\n')
# print(soup.find_all('h1'))

# for tag in soup.find_all(True):
#     print(tag.name)